[
  {
    "objectID": "T1_INTRO_GLM.html",
    "href": "T1_INTRO_GLM.html",
    "title": "Tema 1: Introducción a los modelos generales lineales",
    "section": "",
    "text": "¿Cuál es la probabilidad de aceptar cierta oferta, en función de las características del producto, y del tipo de potenciales clientes que pueden acceder a ella?\n¿Qué cantidad de personas se moverá a lo largo del periodo estival?\n¿Cómo influyen las distintas dosis de un nuevo medicamento en la mejora de los síntomas en pacientes con una condición específica, considerando no solo las características individuales de los pacientes, sino también las variaciones entre diferentes centros de tratamiento?\nEstas preguntas podrían asociarse a un problema de regresión, aunque podemos ver variaciones que puede que no encajen con un modelo de regresión lineal tradicional. En las cuestionesanteriores encontraríamos los siguientes motivos para justivicar esto:\n1. La variable respuesta no es normal.\n2. El rango la variable respuesta no son todos los números reales, como ocurre en el segundo ejemplo, donde el rango son los números enteros no negativos.\n3. Si tenemos en cuenta una estructura jerárquica de la población, un modelo lineal no recoge los efectos de dicha estructura. Esto ocurre en el tercer ejemplo: un paciente se asocia a un centro de tratamiento, pudiendo entender este como un nivel superior.\nRespuesta binaria, de recuento, categórica, o acotada nos indica que puede que un modelo de regresión lineal no sea adecuado. En tal caso se propone el uso de una generalización de la especificación lineal. Estos son los modelos lineales generales, y esto se consigue a través de una función de enlace, o siendo más concretos las componentes de estos modelos son:\n1. Componente aleatoria.\n2. Predictor lineal.\n3. Función de enlace.\nEn este capítulo veremos cada uno de estos componentes, el ajuste de parámetros, inferencia sobre los mismos, el estudio del error, y la selección del modelo.\n\n\nLa componente aleatoria de los modelos que vamos a considerar se ajustan a lo que se conoce como la familia de diespersión exponencial:\n\\[f\\left(y_i ; \\theta_i, \\phi\\right)=\\exp \\left\\{\\frac{y_i \\theta_i-b\\left(\\theta_i\\right)}{a(\\phi)}+c\\left(y_i, \\phi\\right)\\right\\}\\] A \\(\\theta_i\\) se le llama parámetro natural, y a \\(\\phi\\) parámetro de dispersión.\nLa función de verosimilitud, dada una muestra, viene dado por el producto de las funciones de densidad para cada una de las observaciones (en este caso fijas, y no variables) y viendo los parámetros como las variables a ajustar. Por comodidad se toma la función de log-verosimilitud:\n\\[L_i = \\log(f(y_i,\\theta_i,\\phi)) = \\frac{y_i \\theta_i-b\\left(\\theta_i\\right)}{a(\\phi)}+c\\left(y_i, \\phi\\right)\\ \\ \\ \\ \\Longrightarrow\\ \\ \\ \\ \\ \\ L = \\sum_iL_i\\]\nTeniendo en cuenta las condiciones de regularidad para la familia exponencial, al tomar esperanza sobre las derivadas de primer y segundo orden de la función de log-verosimilitud, se llega a que:\n\\[\\begin{array}{lcl}\n\\mu_i = E[y_i] & = & b'(\\theta_i)\\\\\nVar(y_i) & = & b''(\\theta_i)a(\\phi)\n\\end{array}\\]\n\n\n\nLas variables predictoras se relacionan con la variable respuesta a través de la función de enlace. En el caso del enlace canónico tenemos \\[\\eta_i=\\sum_{j=1}^p\\beta_jx_{ij}=g(\\mu_i)\\] ¿Cómo podemos expresar en función de \\(b(·)\\)?\n\n\n\n\nTeniendo en cuenta las relaciones entre \\(\\eta_i=g(\\mu_i)=\\sum_j\\beta_jx_{ij}\\) si calculamos derivadas parciales respecto \\(\\mathrm{\\beta}\\) de la función de log-verosimilitud, e igualamos a cero, el sistema de ecuaciones obtenido son las ECUACIONES DE VEROSIMILITUD, que vienen dadas por:\n\\[\n\\frac{\\partial L(\\mathrm{\\beta})}{\\partial \\beta_j}=\\sum_{i=1}^n\\frac{(y_i-\\mu_i)x_{ij}}{var(y_i)}\\frac{\\partial\\mu_i}{\\partial \\eta_i}=0\\ \\ \\ \\ ,\\ \\ \\ \\ j=1,\\ldots,p.\n\\]\nsiendo \\(p\\) el número de variables predictoras. Se trata de un sistema de ecuaciones, que en general son no lineales, con incógnitas \\(\\beta_j\\): ¿dónde están las incógnitas en el sistema de ecuaciones?\nEn términos matriciales podríamos escribir como: \\[\\mathrm{X^TDV^{-1}}(\\mathrm{y-\\mu})=\\mathrm{0}\\]\ndonde \\(D=\\text{diag}\\{\\frac{\\partial\\mu_i}{\\partial\\eta_i}\\}\\), y \\(V\\) matriz diagonal con las inversas de las varianzas de \\(y_i\\).\n\n\n\nEl tipo de tests en los que nos centramos son:\n\\[\nH_0:\\mathrm{\\Lambda\\beta} =\\mathrm{0}\n\\]\ncombinación lineal de parámetros del modelo. En particular, nos centramos en:\n\nSignificación individual: caso en que \\(\\Lambda = (0,\\ldots,0,1,0,\\ldots,0)\\) todos los valores nulos, salvo la posición \\(j\\), dando lugar a :\\[\nH_0:\\beta_j = 0\\]\nSignificación global: cuando \\(\\Lambda\\) es la matriz identidad de orden \\(p\\). En tal caso se quiere contrastar si \\(\\mathrm{\\beta}\\) es conjuntamente significativo.\n\nPara estos casos, las alternativas que se presentan son 3:\n\nTest de razón de verosimilitudes\nTests de Wald\nTests score\n\nVeamos cada uno de estos 3:\n\n\nEl procedimiento de este test es estimar el cociente entre log-verosimilitudes correspondientes a la hipótesis nula en el numerador, y la alternativa en el denominador. El estadístico usado es:\n\\[\n-2\\log\\Lambda = -2\\log\\left(\\frac{\\text{argmax}_{H_0}\\text{verosim.}}{\\text{argmax}_{H_1}\\text{verosim.}}\\right)=-2(L_0-L_1)\n\\]\ndonde \\(L_0\\) es el valor máximo de la log-verosimilitud para la muestra bajo \\(H_0\\), y \\(L_1\\) es el valor máximo de la log-verosimilitud para la muestra bajo \\(H_1\\). La distribución asintótica de este estadístico bajo \\(H_0\\) es una \\(\\chi^2_1\\), chi cuadrado con un grado de libertad (aproximación de Wilks).\n\n\n\nSe basa en los errores estándar obtenidos de la inversa de la matriz de información, que depende de valores de los parámetros (teóricamente desconocidos):\n\\[\n\\frac{\\beta-\\beta_0}{SE}\n\\]\ncuando la hipótesis planteada es, en caso de parámetros individuales:\n\\[\nH_0: \\beta=\\beta_0\n\\]\nLa aproximación usada es considerar el estadístico anterior sustituyendo el valor desconocido por su estimación máximo verosímil, y en tal caso tendremos un estadístico con distribución conocida independiente de los parámetros:\n\\[\nz =\\frac{\\hat{\\beta}-\\beta_0}{SE} \\sim \\mathcal{N}(0,1)\\ \\ \\ \\text{bajo }\\ H_0\n\\]\nde donde \\(z^2\\) tiene una aproximación asintótica \\(\\chi^2_1\\).\nPara múlitples parámetros, si \\(\\beta = (\\beta_0,\\beta_1)\\) y querremos contrastar que el conjunto de parámetros \\(\\beta_0\\) es nulo, es decir\n\\[H_0 : \\beta_0 = 0\\] el estadístico de Wald adopta la forma:\n\\[\n\\hat{\\beta_0}^T\\left[\\widehat{var}(\\hat{\\beta}_0)\\right]^{-1}\\hat{\\beta}_0\n\\]\nque también sigue una \\(\\chi^2\\) con tantos grados de libertada como el número de parámetros a contrastar.\n\n\n\nEste tests se basa en la pendiente esperada de la curvatura de la función de log-verosimilitud, evaluada en el valor correspondiente a \\(H_0\\). En el caso de parámetros individuales tenemos el siguiente estadístico que sigue una \\(\\chi^2\\)\n\\[\n\\frac{\\left[\\frac{\\partial L(\\beta)}{\\partial\\beta_0}\\right]^2}{-E\\left[\\frac{\\partial L(\\beta)}{\\partial\\beta_0}\\right]^2},\n\\]\nen el caso de multiparámetros tendremos un estadístico que adopta una forma cuadrática basada en el vector de derivadas parciales de la función de verosimilitud, y la inversa de la matriz de información. Será de la forma:\n\\[\n\\mathrm{L·I^{-1}·L}\n\\]\ncuando \\(L\\) sea el vector de derivadas parciales, e \\(I\\) la matriz de información.\n\n\n\n\n\n\n\n\n\nCall:\nglm(formula = count ~ race, family = poisson(link = \"log\"), data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0218  -0.4295  -0.4295  -0.4295   6.1874  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.38321    0.09713  -24.54   &lt;2e-16 ***\nrace1        1.73314    0.14657   11.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 962.80  on 1307  degrees of freedom\nResidual deviance: 844.71  on 1306  degrees of freedom\nAIC: 1122\n\nNumber of Fisher Scoring iterations: 6\n\n\n[1] 1121.99\n\n\n\n\n\n\nEn el contexto de los Modelos Lineales Generalizados (GLM), un modelo saturado se refiere a un modelo que incluye todos los términos posibles en la matriz de diseño. En otras palabras, el modelo saturado tiene un grado de libertad igual al número total de observaciones, lo que significa que tiene un parámetro para cada dato individual. Este tipo de modelo se ajusta perfectamente a los datos observados, y, por lo tanto, tiene cero residuos.\nEn un modelo saturado, la probabilidad ajustada para cada observación es exactamente igual al valor observado. Esto puede ser expresado matemáticamente como (y_i = _i), donde (y_i) es el valor observado y (_i) es el valor predicho por el modelo saturado para la i-ésima observación.\nUsos para la Validación de Modelos:\n\nPrueba de Adecuación del Modelo:\n\nAl comparar un modelo más simple con un modelo saturado, puedes realizar pruebas de hipótesis para evaluar la adecuación del modelo más simple en comparación con el modelo saturado. Esto se hace utilizando estadísticas de prueba como el estadístico de razón de verosimilitud (LRT) o el estadístico de Wald.\n\nComparación de Modelos:\n\nPuedes utilizar el modelo saturado como punto de referencia para comparar la capacidad predictiva de modelos más simples. Si un modelo más simple proporciona ajuste similar al modelo saturado, es probable que sea más parsimonioso y, por lo tanto, preferible.\n\nIdentificación de Problemas:\n\nLa discrepancia entre un modelo saturado y un modelo más simple puede resaltar áreas donde el modelo más simple no se ajusta adecuadamente a los datos. Esto podría indicar problemas con la especificación del modelo o datos atípicos.\n\nAnálisis Residual:\n\nAl comparar los residuos de un modelo más simple con los residuos del modelo saturado, puedes evaluar la presencia de patrones sistemáticos en los residuos que podrían indicar problemas en la especificación del modelo.\n\n\nEl modelo saturado sirve como un punto de referencia útil para evaluar la bondad de ajuste y la validez de modelos más simples en el contexto de los Modelos Lineales Generalizados, y esta línea es la que se va a seguir.\nUsaremos la notación:\n\\[\n\\begin{array}{lcl}\n\\text{Log-verosimilitud del modelo a evaluar} & : & L(\\hat{\\mu},\\mathrm{y})\\\\\n\\text{Log-verosimilitud del modelo saturado} & : & L(\\mathrm{y},\\mathrm{y})\\\\\n\\end{array}\n\\]\nEl estadístico usado es:\n\\[\n-2\\log\\left[\\frac{\\text{max. verosimilitud del modelo a evaluar}}{\\text{max. verosimilitud del modelo saturado}}\\right])= -2[L(\\hat{\\mu};\\mathrm{y})-L(\\mathrm{y},\\mathrm{y})]\n\\]\nSi \\(\\tilde{\\theta}_i\\) denota el parámetro natural para el modelo saturado, y \\(\\hat{\\theta}_i\\) el parámetro natural para el modelo a evaluar, en el caso en que \\(a(\\phi) = \\phi/\\omega_i\\), el estadístico adpota la expresión\n\\[\n2\\sum_i\\omega_i\\left[y_i(\\tilde{\\theta}_i-\\hat{\\theta}_i) - b(\\tilde{\\theta}_i)+b(\\hat{\\theta}_i)\\right]/\\phi=D(\\mathrm{y};\\hat{\\mu})/\\phi\n\\]\nsiendo \\(D(\\mathrm{y};\\hat{\\mu})\\) la deviance.\nComo \\(L(\\hat{\\mu};\\mathrm{y}) \\leq L(\\mathrm{y},\\mathrm{y})\\) entonces \\(D(y;\\hat{\\mu})\\geq 0\\) y cuanta mayor sea la deviance, más pobre será nuestro modelo. El estadístico que se usará será\n\\[\nD(\\mathrm{y};\\hat{\\mu})/\\phi\n\\]\nque sigue una distribución asintótica \\(\\chi^2_k\\) donde \\(k\\) es la diferencia entre el número de parámetros del modelo saturado, y el número de parámetros del modelo a evaluar.\nEn los casos en que \\(\\phi\\) sea conocido, se usa este estadístico para la validación del modelo. Su principal uso es para comparación de modelos, como veremos en próximas secciones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDadas las siguientes distribuciones, probar que pertenecen a la familia exponencial, y calcular su media y varianza a partir de las funciones \\(b(·)\\) y \\(a(·)\\) obtenidas:\n\nDistribución exponencial.\nDistribución geométrica.\nDistribución binomial negativa.\n\n\n\n\nDada la distribución de Poisson:\n\nProbar que pertenece a la familia exponencial, identificando las componentes de la misma.\nDeducir media y varianza a partir de lo obtenido anteriormente.\nDeducir una función de enlace para un modelo GLM con variable respuesta Poisson.\n\n\n\n\nSuponiendo \\(y_1 \\sim \\mathcal{P}\\left(\\lambda_i\\right)\\) con \\(g\\left(\\mu_i\\right)=\\beta_0+\\beta_1 x_i\\) para \\(x_i=1\\) para \\(i=1, \\ldots, n_A\\) y \\(x_1=0\\) para \\(i=n_A+1, \\ldots, n_B\\); observaciones independientes en ambos casos.\nProbar que para la función de enlace logarítmico, las ecuaciones de verosimilitud implican que las medias ajustadas en cada grupo coinciden con las ñmedias muestrales."
  },
  {
    "objectID": "T1_INTRO_GLM.html#introducción",
    "href": "T1_INTRO_GLM.html#introducción",
    "title": "Tema 1: Introducción a los modelos generales lineales",
    "section": "",
    "text": "¿Cuál es la probabilidad de aceptar cierta oferta, en función de las características del producto, y del tipo de potenciales clientes que pueden acceder a ella?\n¿Qué cantidad de personas se moverá a lo largo del periodo estival?\n¿Cómo influyen las distintas dosis de un nuevo medicamento en la mejora de los síntomas en pacientes con una condición específica, considerando no solo las características individuales de los pacientes, sino también las variaciones entre diferentes centros de tratamiento?\nEstas preguntas podrían asociarse a un problema de regresión, aunque podemos ver variaciones que puede que no encajen con un modelo de regresión lineal tradicional. En las cuestionesanteriores encontraríamos los siguientes motivos para justivicar esto:\n1. La variable respuesta no es normal.\n2. El rango la variable respuesta no son todos los números reales, como ocurre en el segundo ejemplo, donde el rango son los números enteros no negativos.\n3. Si tenemos en cuenta una estructura jerárquica de la población, un modelo lineal no recoge los efectos de dicha estructura. Esto ocurre en el tercer ejemplo: un paciente se asocia a un centro de tratamiento, pudiendo entender este como un nivel superior.\nRespuesta binaria, de recuento, categórica, o acotada nos indica que puede que un modelo de regresión lineal no sea adecuado. En tal caso se propone el uso de una generalización de la especificación lineal. Estos son los modelos lineales generales, y esto se consigue a través de una función de enlace, o siendo más concretos las componentes de estos modelos son:\n1. Componente aleatoria.\n2. Predictor lineal.\n3. Función de enlace.\nEn este capítulo veremos cada uno de estos componentes, el ajuste de parámetros, inferencia sobre los mismos, el estudio del error, y la selección del modelo.\n\n\nLa componente aleatoria de los modelos que vamos a considerar se ajustan a lo que se conoce como la familia de diespersión exponencial:\n\\[f\\left(y_i ; \\theta_i, \\phi\\right)=\\exp \\left\\{\\frac{y_i \\theta_i-b\\left(\\theta_i\\right)}{a(\\phi)}+c\\left(y_i, \\phi\\right)\\right\\}\\] A \\(\\theta_i\\) se le llama parámetro natural, y a \\(\\phi\\) parámetro de dispersión.\nLa función de verosimilitud, dada una muestra, viene dado por el producto de las funciones de densidad para cada una de las observaciones (en este caso fijas, y no variables) y viendo los parámetros como las variables a ajustar. Por comodidad se toma la función de log-verosimilitud:\n\\[L_i = \\log(f(y_i,\\theta_i,\\phi)) = \\frac{y_i \\theta_i-b\\left(\\theta_i\\right)}{a(\\phi)}+c\\left(y_i, \\phi\\right)\\ \\ \\ \\ \\Longrightarrow\\ \\ \\ \\ \\ \\ L = \\sum_iL_i\\]\nTeniendo en cuenta las condiciones de regularidad para la familia exponencial, al tomar esperanza sobre las derivadas de primer y segundo orden de la función de log-verosimilitud, se llega a que:\n\\[\\begin{array}{lcl}\n\\mu_i = E[y_i] & = & b'(\\theta_i)\\\\\nVar(y_i) & = & b''(\\theta_i)a(\\phi)\n\\end{array}\\]\n\n\n\nLas variables predictoras se relacionan con la variable respuesta a través de la función de enlace. En el caso del enlace canónico tenemos \\[\\eta_i=\\sum_{j=1}^p\\beta_jx_{ij}=g(\\mu_i)\\] ¿Cómo podemos expresar en función de \\(b(·)\\)?"
  },
  {
    "objectID": "T1_INTRO_GLM.html#ecuaciones-de-verosimilitud",
    "href": "T1_INTRO_GLM.html#ecuaciones-de-verosimilitud",
    "title": "Tema 1: Introducción a los modelos generales lineales",
    "section": "",
    "text": "Teniendo en cuenta las relaciones entre \\(\\eta_i=g(\\mu_i)=\\sum_j\\beta_jx_{ij}\\) si calculamos derivadas parciales respecto \\(\\mathrm{\\beta}\\) de la función de log-verosimilitud, e igualamos a cero, el sistema de ecuaciones obtenido son las ECUACIONES DE VEROSIMILITUD, que vienen dadas por:\n\\[\n\\frac{\\partial L(\\mathrm{\\beta})}{\\partial \\beta_j}=\\sum_{i=1}^n\\frac{(y_i-\\mu_i)x_{ij}}{var(y_i)}\\frac{\\partial\\mu_i}{\\partial \\eta_i}=0\\ \\ \\ \\ ,\\ \\ \\ \\ j=1,\\ldots,p.\n\\]\nsiendo \\(p\\) el número de variables predictoras. Se trata de un sistema de ecuaciones, que en general son no lineales, con incógnitas \\(\\beta_j\\): ¿dónde están las incógnitas en el sistema de ecuaciones?\nEn términos matriciales podríamos escribir como: \\[\\mathrm{X^TDV^{-1}}(\\mathrm{y-\\mu})=\\mathrm{0}\\]\ndonde \\(D=\\text{diag}\\{\\frac{\\partial\\mu_i}{\\partial\\eta_i}\\}\\), y \\(V\\) matriz diagonal con las inversas de las varianzas de \\(y_i\\)."
  },
  {
    "objectID": "T1_INTRO_GLM.html#inferencia-en-el-glm",
    "href": "T1_INTRO_GLM.html#inferencia-en-el-glm",
    "title": "Tema 1: Introducción a los modelos generales lineales",
    "section": "",
    "text": "El tipo de tests en los que nos centramos son:\n\\[\nH_0:\\mathrm{\\Lambda\\beta} =\\mathrm{0}\n\\]\ncombinación lineal de parámetros del modelo. En particular, nos centramos en:\n\nSignificación individual: caso en que \\(\\Lambda = (0,\\ldots,0,1,0,\\ldots,0)\\) todos los valores nulos, salvo la posición \\(j\\), dando lugar a :\\[\nH_0:\\beta_j = 0\\]\nSignificación global: cuando \\(\\Lambda\\) es la matriz identidad de orden \\(p\\). En tal caso se quiere contrastar si \\(\\mathrm{\\beta}\\) es conjuntamente significativo.\n\nPara estos casos, las alternativas que se presentan son 3:\n\nTest de razón de verosimilitudes\nTests de Wald\nTests score\n\nVeamos cada uno de estos 3:\n\n\nEl procedimiento de este test es estimar el cociente entre log-verosimilitudes correspondientes a la hipótesis nula en el numerador, y la alternativa en el denominador. El estadístico usado es:\n\\[\n-2\\log\\Lambda = -2\\log\\left(\\frac{\\text{argmax}_{H_0}\\text{verosim.}}{\\text{argmax}_{H_1}\\text{verosim.}}\\right)=-2(L_0-L_1)\n\\]\ndonde \\(L_0\\) es el valor máximo de la log-verosimilitud para la muestra bajo \\(H_0\\), y \\(L_1\\) es el valor máximo de la log-verosimilitud para la muestra bajo \\(H_1\\). La distribución asintótica de este estadístico bajo \\(H_0\\) es una \\(\\chi^2_1\\), chi cuadrado con un grado de libertad (aproximación de Wilks).\n\n\n\nSe basa en los errores estándar obtenidos de la inversa de la matriz de información, que depende de valores de los parámetros (teóricamente desconocidos):\n\\[\n\\frac{\\beta-\\beta_0}{SE}\n\\]\ncuando la hipótesis planteada es, en caso de parámetros individuales:\n\\[\nH_0: \\beta=\\beta_0\n\\]\nLa aproximación usada es considerar el estadístico anterior sustituyendo el valor desconocido por su estimación máximo verosímil, y en tal caso tendremos un estadístico con distribución conocida independiente de los parámetros:\n\\[\nz =\\frac{\\hat{\\beta}-\\beta_0}{SE} \\sim \\mathcal{N}(0,1)\\ \\ \\ \\text{bajo }\\ H_0\n\\]\nde donde \\(z^2\\) tiene una aproximación asintótica \\(\\chi^2_1\\).\nPara múlitples parámetros, si \\(\\beta = (\\beta_0,\\beta_1)\\) y querremos contrastar que el conjunto de parámetros \\(\\beta_0\\) es nulo, es decir\n\\[H_0 : \\beta_0 = 0\\] el estadístico de Wald adopta la forma:\n\\[\n\\hat{\\beta_0}^T\\left[\\widehat{var}(\\hat{\\beta}_0)\\right]^{-1}\\hat{\\beta}_0\n\\]\nque también sigue una \\(\\chi^2\\) con tantos grados de libertada como el número de parámetros a contrastar.\n\n\n\nEste tests se basa en la pendiente esperada de la curvatura de la función de log-verosimilitud, evaluada en el valor correspondiente a \\(H_0\\). En el caso de parámetros individuales tenemos el siguiente estadístico que sigue una \\(\\chi^2\\)\n\\[\n\\frac{\\left[\\frac{\\partial L(\\beta)}{\\partial\\beta_0}\\right]^2}{-E\\left[\\frac{\\partial L(\\beta)}{\\partial\\beta_0}\\right]^2},\n\\]\nen el caso de multiparámetros tendremos un estadístico que adopta una forma cuadrática basada en el vector de derivadas parciales de la función de verosimilitud, y la inversa de la matriz de información. Será de la forma:\n\\[\n\\mathrm{L·I^{-1}·L}\n\\]\ncuando \\(L\\) sea el vector de derivadas parciales, e \\(I\\) la matriz de información.\n\n\n\n\n\n\n\n\n\nCall:\nglm(formula = count ~ race, family = poisson(link = \"log\"), data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0218  -0.4295  -0.4295  -0.4295   6.1874  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.38321    0.09713  -24.54   &lt;2e-16 ***\nrace1        1.73314    0.14657   11.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 962.80  on 1307  degrees of freedom\nResidual deviance: 844.71  on 1306  degrees of freedom\nAIC: 1122\n\nNumber of Fisher Scoring iterations: 6\n\n\n[1] 1121.99"
  },
  {
    "objectID": "T1_INTRO_GLM.html#deviance-de-un-glm-comparación-y-validación",
    "href": "T1_INTRO_GLM.html#deviance-de-un-glm-comparación-y-validación",
    "title": "Tema 1: Introducción a los modelos generales lineales",
    "section": "",
    "text": "En el contexto de los Modelos Lineales Generalizados (GLM), un modelo saturado se refiere a un modelo que incluye todos los términos posibles en la matriz de diseño. En otras palabras, el modelo saturado tiene un grado de libertad igual al número total de observaciones, lo que significa que tiene un parámetro para cada dato individual. Este tipo de modelo se ajusta perfectamente a los datos observados, y, por lo tanto, tiene cero residuos.\nEn un modelo saturado, la probabilidad ajustada para cada observación es exactamente igual al valor observado. Esto puede ser expresado matemáticamente como (y_i = _i), donde (y_i) es el valor observado y (_i) es el valor predicho por el modelo saturado para la i-ésima observación.\nUsos para la Validación de Modelos:\n\nPrueba de Adecuación del Modelo:\n\nAl comparar un modelo más simple con un modelo saturado, puedes realizar pruebas de hipótesis para evaluar la adecuación del modelo más simple en comparación con el modelo saturado. Esto se hace utilizando estadísticas de prueba como el estadístico de razón de verosimilitud (LRT) o el estadístico de Wald.\n\nComparación de Modelos:\n\nPuedes utilizar el modelo saturado como punto de referencia para comparar la capacidad predictiva de modelos más simples. Si un modelo más simple proporciona ajuste similar al modelo saturado, es probable que sea más parsimonioso y, por lo tanto, preferible.\n\nIdentificación de Problemas:\n\nLa discrepancia entre un modelo saturado y un modelo más simple puede resaltar áreas donde el modelo más simple no se ajusta adecuadamente a los datos. Esto podría indicar problemas con la especificación del modelo o datos atípicos.\n\nAnálisis Residual:\n\nAl comparar los residuos de un modelo más simple con los residuos del modelo saturado, puedes evaluar la presencia de patrones sistemáticos en los residuos que podrían indicar problemas en la especificación del modelo.\n\n\nEl modelo saturado sirve como un punto de referencia útil para evaluar la bondad de ajuste y la validez de modelos más simples en el contexto de los Modelos Lineales Generalizados, y esta línea es la que se va a seguir.\nUsaremos la notación:\n\\[\n\\begin{array}{lcl}\n\\text{Log-verosimilitud del modelo a evaluar} & : & L(\\hat{\\mu},\\mathrm{y})\\\\\n\\text{Log-verosimilitud del modelo saturado} & : & L(\\mathrm{y},\\mathrm{y})\\\\\n\\end{array}\n\\]\nEl estadístico usado es:\n\\[\n-2\\log\\left[\\frac{\\text{max. verosimilitud del modelo a evaluar}}{\\text{max. verosimilitud del modelo saturado}}\\right])= -2[L(\\hat{\\mu};\\mathrm{y})-L(\\mathrm{y},\\mathrm{y})]\n\\]\nSi \\(\\tilde{\\theta}_i\\) denota el parámetro natural para el modelo saturado, y \\(\\hat{\\theta}_i\\) el parámetro natural para el modelo a evaluar, en el caso en que \\(a(\\phi) = \\phi/\\omega_i\\), el estadístico adpota la expresión\n\\[\n2\\sum_i\\omega_i\\left[y_i(\\tilde{\\theta}_i-\\hat{\\theta}_i) - b(\\tilde{\\theta}_i)+b(\\hat{\\theta}_i)\\right]/\\phi=D(\\mathrm{y};\\hat{\\mu})/\\phi\n\\]\nsiendo \\(D(\\mathrm{y};\\hat{\\mu})\\) la deviance.\nComo \\(L(\\hat{\\mu};\\mathrm{y}) \\leq L(\\mathrm{y},\\mathrm{y})\\) entonces \\(D(y;\\hat{\\mu})\\geq 0\\) y cuanta mayor sea la deviance, más pobre será nuestro modelo. El estadístico que se usará será\n\\[\nD(\\mathrm{y};\\hat{\\mu})/\\phi\n\\]\nque sigue una distribución asintótica \\(\\chi^2_k\\) donde \\(k\\) es la diferencia entre el número de parámetros del modelo saturado, y el número de parámetros del modelo a evaluar.\nEn los casos en que \\(\\phi\\) sea conocido, se usa este estadístico para la validación del modelo. Su principal uso es para comparación de modelos, como veremos en próximas secciones."
  },
  {
    "objectID": "T1_INTRO_GLM.html#ejercicios",
    "href": "T1_INTRO_GLM.html#ejercicios",
    "title": "Tema 1: Introducción a los modelos generales lineales",
    "section": "",
    "text": "Dadas las siguientes distribuciones, probar que pertenecen a la familia exponencial, y calcular su media y varianza a partir de las funciones \\(b(·)\\) y \\(a(·)\\) obtenidas:\n\nDistribución exponencial.\nDistribución geométrica.\nDistribución binomial negativa.\n\n\n\n\nDada la distribución de Poisson:\n\nProbar que pertenece a la familia exponencial, identificando las componentes de la misma.\nDeducir media y varianza a partir de lo obtenido anteriormente.\nDeducir una función de enlace para un modelo GLM con variable respuesta Poisson.\n\n\n\n\nSuponiendo \\(y_1 \\sim \\mathcal{P}\\left(\\lambda_i\\right)\\) con \\(g\\left(\\mu_i\\right)=\\beta_0+\\beta_1 x_i\\) para \\(x_i=1\\) para \\(i=1, \\ldots, n_A\\) y \\(x_1=0\\) para \\(i=n_A+1, \\ldots, n_B\\); observaciones independientes en ambos casos.\nProbar que para la función de enlace logarítmico, las ecuaciones de verosimilitud implican que las medias ajustadas en cada grupo coinciden con las ñmedias muestrales."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Thanks for checking out my web site!"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  }
]