---
title: "Tema 1: Introducción a los modelos generales lineales"
toc: true
toc_float:
execute: 
  collapsed: false
output:
  quarto::quarto_document:
    toc: true
    toc_float:
      collapsed: true
---

# TEMA 1: INTRODUCCIÓN A GLM

## INTRODUCCIÓN

**¿Cuál es la probabilidad de aceptar cierta oferta, en función de las características del producto, y del tipo de potenciales clientes que pueden acceder a ella?**

**¿Qué cantidad de personas se moverá a lo largo del periodo estival?**

**¿Cómo influyen las distintas dosis de un nuevo medicamento en la mejora de los síntomas en pacientes con una condición específica, considerando no solo las características individuales de los pacientes, sino también las variaciones entre diferentes centros de tratamiento?**

Estas preguntas podrían asociarse a un problema de regresión, aunque podemos ver variaciones que puede que no encajen con un modelo de regresión lineal tradicional. En las cuestionesanteriores encontraríamos los siguientes motivos para justivicar esto:

*1. La variable respuesta no es normal.*

*2. El rango la variable respuesta no son todos los números reales, como ocurre en el segundo ejemplo, donde el rango son los números enteros no negativos.*

*3. Si tenemos en cuenta una estructura jerárquica de la población, un modelo lineal no recoge los efectos de dicha estructura. Esto ocurre en el tercer ejemplo: un paciente se asocia a un centro de tratamiento, pudiendo entender este como un nivel superior.*

Respuesta binaria, de recuento, categórica, o acotada nos indica que puede que un modelo de regresión lineal no sea adecuado. En tal caso se propone el uso de una generalización de la especificación lineal. Estos son los modelos lineales generales, y esto se consigue a través de una función de enlace, o siendo más concretos las componentes de estos modelos son:

***1. Componente aleatoria.***

***2. Predictor lineal.***

***3. Función de enlace.***

En este capítulo veremos cada uno de estos componentes, el ajuste de parámetros, inferencia sobre los mismos, el estudio del error, y la selección del modelo.

### COMPONENTE ALEATORIA: FAMILIA DE DISPERSIÓN EXPONENCIAL

La componente aleatoria de los modelos que vamos a considerar se ajustan a lo que se conoce como la familia de diespersión exponencial:

$$f\left(y_i ; \theta_i, \phi\right)=\exp \left\{\frac{y_i \theta_i-b\left(\theta_i\right)}{a(\phi)}+c\left(y_i, \phi\right)\right\}$$ A $\theta_i$ se le llama **parámetro natural**, y a $\phi$ **parámetro de dispersión**.

La función de verosimilitud, dada una muestra, viene dado por el producto de las funciones de densidad para cada una de las observaciones (en este caso fijas, y no variables) y viendo los parámetros como las variables a ajustar. Por comodidad se toma la **función de log-verosimilitud:**

$$L_i = \log(f(y_i,\theta_i,\phi)) = \frac{y_i \theta_i-b\left(\theta_i\right)}{a(\phi)}+c\left(y_i, \phi\right)\ \ \ \ \Longrightarrow\ \ \ \ \ \ L = \sum_iL_i$$

Teniendo en cuenta las condiciones de regularidad para la familia exponencial, al tomar esperanza sobre las derivadas de primer y segundo orden de la función de log-verosimilitud, se llega a que:

$$\begin{array}{lcl}
\mu_i = E[y_i] & = & b'(\theta_i)\\
Var(y_i) & = & b''(\theta_i)a(\phi)
\end{array}$$

### FUNCIÓN DE ENLACE CANÓNICA Y PREDICTOR LINEAL

Las variables predictoras se relacionan con la variable respuesta a través de la función de enlace. En el caso del *enlace canónico* tenemos $$\eta_i=\sum_{j=1}^p\beta_jx_{ij}=g(\mu_i)$$ **¿Cómo podemos expresar en función de** $b(·)$?

## ECUACIONES DE VEROSIMILITUD

Teniendo en cuenta las relaciones entre $\eta_i=g(\mu_i)=\sum_j\beta_jx_{ij}$ si calculamos derivadas parciales respecto $\mathrm{\beta}$ de la función de log-verosimilitud, e igualamos a cero, el sistema de ecuaciones obtenido son las **ECUACIONES DE VEROSIMILITUD**, que vienen dadas por:

$$
\frac{\partial L(\mathrm{\beta})}{\partial \beta_j}=\sum_{i=1}^n\frac{(y_i-\mu_i)x_{ij}}{var(y_i)}\frac{\partial\mu_i}{\partial \eta_i}=0\ \ \ \ ,\ \ \ \ j=1,\ldots,p.
$$

siendo $p$ el número de variables predictoras. Se trata de un sistema de ecuaciones, que en general son no lineales, con incógnitas $\beta_j$: **¿dónde están las incógnitas en el sistema de ecuaciones?**

En términos matriciales podríamos escribir como: $$\mathrm{X^TDV^{-1}}(\mathrm{y-\mu})=\mathrm{0}$$

donde $D=\text{diag}\{\frac{\partial\mu_i}{\partial\eta_i}\}$, y $V$ matriz diagonal con las inversas de las varianzas de $y_i$.

## INFERENCIA EN EL GLM

El tipo de tests en los que nos centramos son:

$$
H_0:\mathrm{\Lambda\beta} =\mathrm{0}
$$

combinación lineal de parámetros del modelo. En particular, nos centramos en:

1.  **Significación individual:** caso en que $\Lambda = (0,\ldots,0,1,0,\ldots,0)$ todos los valores nulos, salvo la posición $j$, dando lugar a :$$
    H_0:\beta_j = 0$$
2.  **Significación global**: cuando $\Lambda$ es la matriz identidad de orden $p$. En tal caso se quiere contrastar si $\mathrm{\beta}$ es conjuntamente significativo.

Para estos casos, las alternativas que se presentan son 3:

1.  **Test de razón de verosimilitudes**
2.  **Tests de Wald**
3.  **Tests score**

Veamos cada uno de estos 3:

### TEST DE RAZÓN DE VEROSIMILITUDES

El procedimiento de este test es estimar el cociente entre log-verosimilitudes correspondientes a la hipótesis nula en el numerador, y la alternativa en el denominador. El estadístico usado es:

$$
-2\log\Lambda = -2\log\left(\frac{\text{argmax}_{H_0}\text{verosim.}}{\text{argmax}_{H_1}\text{verosim.}}\right)=-2(L_0-L_1)
$$

donde $L_0$ es el valor máximo de la log-verosimilitud para la muestra bajo $H_0$, y $L_1$ es el valor máximo de la log-verosimilitud para la muestra bajo $H_1$. La distribución asintótica de este estadístico bajo $H_0$ es una $\chi^2_1$, chi cuadrado con un grado de libertad (aproximación de Wilks).

### TESTS DE WALD

Se basa en los errores estándar obtenidos de la inversa de la matriz de información, que depende de valores de los parámetros (teóricamente desconocidos):

$$
\frac{\beta-\beta_0}{SE}
$$

cuando la hipótesis planteada es, en caso de parámetros individuales:

$$
H_0: \beta=\beta_0
$$

La aproximación usada es considerar el estadístico anterior sustituyendo el valor desconocido por su estimación máximo verosímil, y en tal caso tendremos un estadístico con distribución conocida independiente de los parámetros:

$$
z =\frac{\hat{\beta}-\beta_0}{SE} \sim \mathcal{N}(0,1)\ \ \ \text{bajo }\ H_0
$$

de donde $z^2$ tiene una aproximación asintótica $\chi^2_1$.

Para *múlitples parámetros*, si $\beta = (\beta_0,\beta_1)$ y querremos contrastar que el conjunto de parámetros $\beta_0$ es nulo, es decir

$$H_0 : \beta_0 = 0$$ el estadístico de Wald adopta la forma:

$$
\hat{\beta_0}^T\left[\widehat{var}(\hat{\beta}_0)\right]^{-1}\hat{\beta}_0
$$

que también sigue una $\chi^2$ con tantos grados de libertada como el número de parámetros a contrastar.

### TEST SCORE

Este tests se basa en la pendiente esperada de la curvatura de la función de log-verosimilitud, evaluada en el valor correspondiente a $H_0$. En el caso de parámetros individuales tenemos el siguiente estadístico que sigue una $\chi^2$

$$
\frac{\left[\frac{\partial L(\beta)}{\partial\beta_0}\right]^2}{-E\left[\frac{\partial L(\beta)}{\partial\beta_0}\right]^2},
$$

en el caso de multiparámetros tendremos un estadístico que adopta una forma cuadrática basada en el vector de derivadas parciales de la función de verosimilitud, y la inversa de la matriz de información. Será de la forma:

$$
\mathrm{L·I^{-1}·L}
$$

cuando $L$ sea el vector de derivadas parciales, e $I$ la matriz de información.

#### EJERCICIO: Deduce los estadísticos anteriores en el caso de un modelo binomial, con proporción muestral $\hat{\pi}=y$

### INTERVALOS DE CONFIANZA A PARTIR DE LA INVERSIÓN DE TESTS

```{r,echo=FALSE}
url = "http://users.stat.ufl.edu/~aa/glm/data/Homicides.dat"
data = read.table(url, header=TRUE)
data = data[,c("race", "count")] #removing observation number
data$race=as.factor(data$race)

# MODELO DE POISSON:
poisson.model <- glm(count~race, family=poisson(link="log"),data=data)
summary(poisson.model)
poisson.model$aic
```

## DEVIANCE DE UN GLM, COMPARACIÓN Y VALIDACIÓN

En el contexto de los Modelos Lineales Generalizados (GLM), un modelo saturado se refiere a un modelo que incluye todos los términos posibles en la matriz de diseño. En otras palabras, el modelo saturado tiene un grado de libertad igual al número total de observaciones, lo que significa que tiene un parámetro para cada dato individual. Este tipo de modelo se ajusta perfectamente a los datos observados, y, por lo tanto, tiene cero residuos.

En un modelo saturado, la probabilidad ajustada para cada observación es exactamente igual al valor observado. Esto puede ser expresado matemáticamente como (y_i = \hat{y}\_i), donde (y_i) es el valor observado y (\hat{y}\_i) es el valor predicho por el modelo saturado para la i-ésima observación.

**Usos para la Validación de Modelos:**

1.  **Prueba de Adecuación del Modelo:**
    -   Al comparar un modelo más simple con un modelo saturado, puedes realizar pruebas de hipótesis para evaluar la adecuación del modelo más simple en comparación con el modelo saturado. Esto se hace utilizando estadísticas de prueba como el estadístico de razón de verosimilitud (LRT) o el estadístico de Wald.
2.  **Comparación de Modelos:**
    -   Puedes utilizar el modelo saturado como punto de referencia para comparar la capacidad predictiva de modelos más simples. Si un modelo más simple proporciona ajuste similar al modelo saturado, es probable que sea más parsimonioso y, por lo tanto, preferible.
3.  **Identificación de Problemas:**
    -   La discrepancia entre un modelo saturado y un modelo más simple puede resaltar áreas donde el modelo más simple no se ajusta adecuadamente a los datos. Esto podría indicar problemas con la especificación del modelo o datos atípicos.
4.  **Análisis Residual:**
    -   Al comparar los residuos de un modelo más simple con los residuos del modelo saturado, puedes evaluar la presencia de patrones sistemáticos en los residuos que podrían indicar problemas en la especificación del modelo.

El modelo saturado sirve como un punto de referencia útil para evaluar la bondad de ajuste y la validez de modelos más simples en el contexto de los Modelos Lineales Generalizados, y esta línea es la que se va a seguir.

Usaremos la notación:

$$
\begin{array}{lcl}
\text{Log-verosimilitud del modelo a evaluar} & : & L(\hat{\mu},\mathrm{y})\\
\text{Log-verosimilitud del modelo saturado} & : & L(\mathrm{y},\mathrm{y})\\
\end{array}
$$

El estadístico usado es:

$$
-2\log\left[\frac{\text{max. verosimilitud del modelo a evaluar}}{\text{max. verosimilitud del modelo saturado}}\right])= -2[L(\hat{\mu};\mathrm{y})-L(\mathrm{y},\mathrm{y})]
$$

Si $\tilde{\theta}_i$ denota el parámetro natural para el modelo saturado, y $\hat{\theta}_i$ el parámetro natural para el modelo a evaluar, en el caso en que $a(\phi) = \phi/\omega_i$, el estadístico adpota la expresión

$$
2\sum_i\omega_i\left[y_i(\tilde{\theta}_i-\hat{\theta}_i) - b(\tilde{\theta}_i)+b(\hat{\theta}_i)\right]/\phi=D(\mathrm{y};\hat{\mu})/\phi
$$

siendo $D(\mathrm{y};\hat{\mu})$ la **deviance.**

Como $L(\hat{\mu};\mathrm{y}) \leq L(\mathrm{y},\mathrm{y})$ entonces $D(y;\hat{\mu})\geq 0$ y **cuanta mayor sea la deviance, más pobre será nuestro modelo**. El estadístico que se usará será

$$
D(\mathrm{y};\hat{\mu})/\phi
$$

que sigue una distribución asintótica $\chi^2_k$ donde $k$ es *la diferencia entre el número de parámetros del modelo saturado, y el número de parámetros del modelo a evaluar*.

En los casos en que $\phi$ sea conocido, se usa este estadístico para la validación del modelo. Su principal uso es para comparación de modelos, como veremos en próximas secciones.

#### EJERCICIO: Calcula la deviance para el modelo de Poisson y para el model normal.

## SELECCIÓN DE VARIABLES EXPLICATIVAS

## EJEMPLO COMPLETO

## EJERCICIOS

### EJERCICIO 1

Dadas las siguientes distribuciones, probar que pertenecen a la familia exponencial, y calcular su media y varianza a partir de las funciones $b(·)$ y $a(·)$ obtenidas:

1.  Distribución exponencial.
2.  Distribución geométrica.
3.  Distribución binomial negativa.

### EJERCICIO 2

Dada la distribución de Poisson:

1.  Probar que pertenece a la familia exponencial, identificando las componentes de la misma.
2.  Deducir media y varianza a partir de lo obtenido anteriormente.
3.  Deducir una función de enlace para un modelo GLM con variable respuesta Poisson.

### EJERCICIO 3 (AGRESTI 4.6)

Suponiendo $y_1 \sim \mathcal{P}\left(\lambda_i\right)$ con $g\left(\mu_i\right)=\beta_0+\beta_1 x_i$ para $x_i=1$ para $i=1, \ldots, n_A$ y $x_1=0$ para $i=n_A+1, \ldots, n_B$; observaciones independientes en ambos casos.

Probar que para la función de enlace logarítmico, las ecuaciones de verosimilitud implican que las medias ajustadas en cada grupo coinciden con las ñmedias muestrales.
